{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda\n",
      "Epoch: 0\n",
      "Batch idx: 0, Train-Loss: 2.306, Train-Acc: 0.08\n",
      "Batch idx: 100, Train-Loss: 1.621, Train-Acc: 0.31\n",
      "Batch idx: 200, Train-Loss: 1.300, Train-Acc: 0.59\n",
      "Batch idx: 300, Train-Loss: 1.555, Train-Acc: 0.42\n",
      "Batch idx: 400, Train-Loss: 1.193, Train-Acc: 0.53\n",
      "Batch idx: 500, Train-Loss: 1.392, Train-Acc: 0.58\n",
      "Batch idx: 600, Train-Loss: 1.359, Train-Acc: 0.52\n",
      "Batch idx: 700, Train-Loss: 0.980, Train-Acc: 0.72\n",
      "Test-Loss: 1.258, Test-Acc: 0.55\n",
      "Epoch: 1\n",
      "Batch idx: 0, Train-Loss: 1.270, Train-Acc: 0.53\n",
      "Batch idx: 100, Train-Loss: 0.932, Train-Acc: 0.69\n",
      "Batch idx: 200, Train-Loss: 0.856, Train-Acc: 0.70\n",
      "Batch idx: 300, Train-Loss: 0.953, Train-Acc: 0.70\n",
      "Batch idx: 400, Train-Loss: 1.175, Train-Acc: 0.62\n",
      "Batch idx: 500, Train-Loss: 1.109, Train-Acc: 0.62\n",
      "Batch idx: 600, Train-Loss: 0.938, Train-Acc: 0.70\n",
      "Batch idx: 700, Train-Loss: 1.016, Train-Acc: 0.66\n",
      "Test-Loss: 1.102, Test-Acc: 0.61\n",
      "Epoch: 2\n",
      "Batch idx: 0, Train-Loss: 1.043, Train-Acc: 0.62\n",
      "Batch idx: 100, Train-Loss: 0.866, Train-Acc: 0.75\n",
      "Batch idx: 200, Train-Loss: 1.052, Train-Acc: 0.62\n",
      "Batch idx: 300, Train-Loss: 0.914, Train-Acc: 0.66\n",
      "Batch idx: 400, Train-Loss: 0.918, Train-Acc: 0.73\n",
      "Batch idx: 500, Train-Loss: 0.705, Train-Acc: 0.78\n",
      "Batch idx: 600, Train-Loss: 0.996, Train-Acc: 0.66\n",
      "Batch idx: 700, Train-Loss: 1.051, Train-Acc: 0.55\n",
      "Test-Loss: 1.017, Test-Acc: 0.65\n",
      "Epoch: 3\n",
      "Batch idx: 0, Train-Loss: 0.728, Train-Acc: 0.78\n",
      "Batch idx: 100, Train-Loss: 0.881, Train-Acc: 0.69\n",
      "Batch idx: 200, Train-Loss: 1.297, Train-Acc: 0.62\n",
      "Batch idx: 300, Train-Loss: 1.162, Train-Acc: 0.67\n",
      "Batch idx: 400, Train-Loss: 1.110, Train-Acc: 0.64\n",
      "Batch idx: 500, Train-Loss: 0.721, Train-Acc: 0.77\n",
      "Batch idx: 600, Train-Loss: 0.898, Train-Acc: 0.64\n",
      "Batch idx: 700, Train-Loss: 1.007, Train-Acc: 0.56\n",
      "Test-Loss: 0.964, Test-Acc: 0.66\n",
      "Epoch: 4\n",
      "Batch idx: 0, Train-Loss: 0.886, Train-Acc: 0.75\n",
      "Batch idx: 100, Train-Loss: 0.867, Train-Acc: 0.70\n",
      "Batch idx: 200, Train-Loss: 0.725, Train-Acc: 0.80\n",
      "Batch idx: 300, Train-Loss: 0.985, Train-Acc: 0.73\n",
      "Batch idx: 400, Train-Loss: 0.869, Train-Acc: 0.69\n",
      "Batch idx: 500, Train-Loss: 0.909, Train-Acc: 0.67\n",
      "Batch idx: 600, Train-Loss: 0.919, Train-Acc: 0.72\n",
      "Batch idx: 700, Train-Loss: 0.690, Train-Acc: 0.83\n",
      "Test-Loss: 0.988, Test-Acc: 0.66\n",
      "Epoch: 5\n",
      "Batch idx: 0, Train-Loss: 0.938, Train-Acc: 0.72\n",
      "Batch idx: 100, Train-Loss: 0.841, Train-Acc: 0.73\n",
      "Batch idx: 200, Train-Loss: 0.742, Train-Acc: 0.78\n",
      "Batch idx: 300, Train-Loss: 0.938, Train-Acc: 0.69\n",
      "Batch idx: 400, Train-Loss: 0.667, Train-Acc: 0.77\n",
      "Batch idx: 500, Train-Loss: 0.634, Train-Acc: 0.78\n",
      "Batch idx: 600, Train-Loss: 1.048, Train-Acc: 0.58\n",
      "Batch idx: 700, Train-Loss: 0.920, Train-Acc: 0.69\n",
      "Test-Loss: 0.924, Test-Acc: 0.68\n",
      "Epoch: 6\n",
      "Batch idx: 0, Train-Loss: 0.815, Train-Acc: 0.70\n",
      "Batch idx: 100, Train-Loss: 0.883, Train-Acc: 0.66\n",
      "Batch idx: 200, Train-Loss: 0.631, Train-Acc: 0.80\n",
      "Batch idx: 300, Train-Loss: 0.677, Train-Acc: 0.73\n",
      "Batch idx: 400, Train-Loss: 0.637, Train-Acc: 0.81\n",
      "Batch idx: 500, Train-Loss: 0.912, Train-Acc: 0.69\n",
      "Batch idx: 600, Train-Loss: 0.862, Train-Acc: 0.73\n",
      "Batch idx: 700, Train-Loss: 1.141, Train-Acc: 0.69\n",
      "Test-Loss: 0.913, Test-Acc: 0.69\n",
      "Epoch: 7\n",
      "Batch idx: 0, Train-Loss: 0.578, Train-Acc: 0.84\n",
      "Batch idx: 100, Train-Loss: 0.743, Train-Acc: 0.77\n",
      "Batch idx: 200, Train-Loss: 0.798, Train-Acc: 0.69\n",
      "Batch idx: 300, Train-Loss: 0.898, Train-Acc: 0.73\n",
      "Batch idx: 400, Train-Loss: 0.648, Train-Acc: 0.80\n",
      "Batch idx: 500, Train-Loss: 0.920, Train-Acc: 0.78\n",
      "Batch idx: 600, Train-Loss: 0.659, Train-Acc: 0.77\n",
      "Batch idx: 700, Train-Loss: 0.704, Train-Acc: 0.83\n",
      "Test-Loss: 0.906, Test-Acc: 0.69\n",
      "Epoch: 8\n",
      "Batch idx: 0, Train-Loss: 0.756, Train-Acc: 0.73\n",
      "Batch idx: 100, Train-Loss: 0.829, Train-Acc: 0.72\n",
      "Batch idx: 200, Train-Loss: 0.716, Train-Acc: 0.75\n",
      "Batch idx: 300, Train-Loss: 0.699, Train-Acc: 0.77\n",
      "Batch idx: 400, Train-Loss: 1.016, Train-Acc: 0.62\n",
      "Batch idx: 500, Train-Loss: 0.700, Train-Acc: 0.84\n",
      "Batch idx: 600, Train-Loss: 0.882, Train-Acc: 0.70\n",
      "Batch idx: 700, Train-Loss: 0.592, Train-Acc: 0.80\n",
      "Test-Loss: 0.911, Test-Acc: 0.70\n",
      "Epoch: 9\n",
      "Batch idx: 0, Train-Loss: 0.646, Train-Acc: 0.75\n",
      "Batch idx: 100, Train-Loss: 0.906, Train-Acc: 0.62\n",
      "Batch idx: 200, Train-Loss: 0.598, Train-Acc: 0.75\n",
      "Batch idx: 300, Train-Loss: 1.085, Train-Acc: 0.66\n",
      "Batch idx: 400, Train-Loss: 0.516, Train-Acc: 0.80\n",
      "Batch idx: 500, Train-Loss: 0.857, Train-Acc: 0.80\n",
      "Batch idx: 600, Train-Loss: 0.540, Train-Acc: 0.81\n",
      "Batch idx: 700, Train-Loss: 0.654, Train-Acc: 0.72\n",
      "Test-Loss: 0.895, Test-Acc: 0.70\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(16,32,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc = nn.Linear(32*8*8, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = Model().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for batch_idx, (X, Yd) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        X = X.to(device)\n",
    "        Yd = Yd.to(device)\n",
    "        Y_logit = model(X)\n",
    "        loss = F.cross_entropy(Y_logit, Yd)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            Y = torch.argmax(Y_logit, dim=1)\n",
    "            correct = (Yd == Y).sum().item() \n",
    "            total = Yd.size(0)\n",
    "            acc = correct / total\n",
    "            print(f\"Batch idx: {batch_idx}, Train-Loss: {loss.item():.3f}, Train-Acc: {acc:.2f}\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        loss_all = 0\n",
    "        correct, total = 0, 0\n",
    "        for X, Yd in test_loader:\n",
    "            X = X.to(device)\n",
    "            Yd = Yd.to(device)\n",
    "            Y_logit = model(X)\n",
    "            loss = F.cross_entropy(Y_logit, Yd)\n",
    "            loss_all += loss.item()\n",
    "            Y = torch.argmax(Y_logit, dim=1)\n",
    "            correct += (Yd == Y).sum().item() \n",
    "            total += Yd.size(0)\n",
    "        loss_all /= len(test_loader)\n",
    "        acc = correct / total\n",
    "        print(f\"Test-Loss: {loss_all:.3f}, Test-Acc: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
